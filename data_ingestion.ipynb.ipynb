{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Custom Search"
      ],
      "metadata": {
        "id": "bvbvUyjXReoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install requests beautifulsoup4 pandas pandas-datareader google-api-python-client"
      ],
      "metadata": {
        "id": "iKnU9Te8Eh-P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "SEARCH_ENGINE_ID = userdata.get('SEARCH_ENGINE_ID')"
      ],
      "metadata": {
        "id": "o1thZqjyM4W1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "query = \"S&P 500 companies\"  # Replace with your desired search term\n",
        "url = f\"{base_url}?key={GOOGLE_API_KEY}&cx={SEARCH_ENGINE_ID}&q={query}\""
      ],
      "metadata": {
        "id": "TgeZSymrOybh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import pandas_datareader as pdr\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError"
      ],
      "metadata": {
        "id": "WGZigTu8HB5F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the custom search function\n",
        "\"\"\"def test_custom_search():\n",
        "    try:\n",
        "        print(f\"API Key: {GOOGLE_API_KEY[:5]}...{GOOGLE_API_KEY[-5:]}\")  # Print first and last 5 characters\n",
        "        print(f\"Search Engine ID: {SEARCH_ENGINE_ID}\")\n",
        "\n",
        "        service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
        "\n",
        "        res = service.cse().list(q=\"Google\", cx=SEARCH_ENGINE_ID, num=1).execute()\n",
        "\n",
        "        if 'items' in res:\n",
        "            print(f\"Search successful. First result: {res['items'][0]['link']}\")\n",
        "        else:\n",
        "            print(\"Search successful, but no results found.\")\n",
        "\n",
        "    except HttpError as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        if hasattr(e, 'error_details'):\n",
        "            print(f\"Error details: {e.error_details}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_custom_search()\"\"\""
      ],
      "metadata": {
        "id": "UhBI9_CqNI7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Custom Search Function"
      ],
      "metadata": {
        "id": "RTgvaSQcRjSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "SEARCH_ENGINE_ID = userdata.get('SEARCH_ENGINE_ID')"
      ],
      "metadata": {
        "id": "zNK3tdAjRq6p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "from urllib.parse import urlparse, urljoin\n",
        "import time\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from google.colab import userdata\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def validate_credentials() -> bool:\n",
        "    \"\"\"Validate that required API credentials are present\"\"\"\n",
        "    if not GOOGLE_API_KEY or not SEARCH_ENGINE_ID:\n",
        "        logger.error(\"Missing required API credentials\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def get_sp500_companies() -> List[str]:\n",
        "    \"\"\"Fetch list of S&P 500 companies from Wikipedia\"\"\"\n",
        "    try:\n",
        "        table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "        df = table[0]\n",
        "        return df['Security'].tolist()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching S&P 500 companies: {e}\")\n",
        "        return []\n",
        "\n",
        "def search_company_website(company_name: str) -> Optional[str]:\n",
        "    \"\"\"Search for company's official website using Google Custom Search API\"\"\"\n",
        "    try:\n",
        "        service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
        "        res = service.cse().list(\n",
        "            q=f\"{company_name} official website\",\n",
        "            cx=SEARCH_ENGINE_ID,\n",
        "            num=1\n",
        "        ).execute()\n",
        "\n",
        "        if 'items' in res:\n",
        "            url = res['items'][0]['link']\n",
        "            parsed_url = urlparse(url)\n",
        "            base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "            return base_url\n",
        "\n",
        "        logger.warning(f\"No results found for {company_name}\")\n",
        "        return None\n",
        "\n",
        "    except HttpError as e:\n",
        "        logger.error(f\"Google API error for {company_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "def find_terms_link(url: str) -> Optional[str]:\n",
        "    \"\"\"Find terms and conditions link on website\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10, headers={\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        potential_links = soup.find_all('a', text=lambda text: text and (\n",
        "            'terms' in text.lower() or\n",
        "            'conditions' in text.lower() or\n",
        "            'legal' in text.lower()\n",
        "        ))\n",
        "\n",
        "        if potential_links:\n",
        "            return urljoin(url, potential_links[0]['href'])\n",
        "        return None\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        logger.error(f\"Error finding terms link for {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_terms(url: str, company_name: str) -> bool:\n",
        "    \"\"\"Download and save terms and conditions\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            url,\n",
        "            timeout=10,\n",
        "            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "\n",
        "        content_type = response.headers.get('Content-Type', '').lower()\n",
        "        if 'text' not in content_type:\n",
        "            logger.warning(f\"Skipping non-text content for {company_name}: {content_type}\")\n",
        "            return False\n",
        "\n",
        "        # Create terms directory using pathlib\n",
        "        terms_dir = Path('terms')\n",
        "        terms_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Sanitize filename\n",
        "        safe_company_name = \"\".join(c for c in company_name if c.isalnum() or c in (' ', '-', '_')).strip()\n",
        "        filename = terms_dir / f\"{safe_company_name}_terms.txt\"\n",
        "\n",
        "        # Save the content\n",
        "        filename.write_text(response.text, encoding='utf-8')\n",
        "        logger.info(f\"Saved terms for {company_name} to {filename}\")\n",
        "        return True\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        logger.error(f\"Error downloading terms for {company_name}: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Validate credentials first\n",
        "    if not validate_credentials():\n",
        "        return\n",
        "\n",
        "    # Get S&P 500 companies\n",
        "    sp_500_companies = get_sp500_companies()\n",
        "    if not sp_500_companies:\n",
        "        logger.error(\"Failed to fetch company list\")\n",
        "        return\n",
        "\n",
        "    # Process each company\n",
        "    for i, company in enumerate(sp_500_companies[44:], 44):\n",
        "        logger.info(f\"Processing {company} (#{i})...\")\n",
        "\n",
        "        try:\n",
        "            # Search for company website\n",
        "            website_url = search_company_website(company)\n",
        "            if not website_url:\n",
        "                continue\n",
        "\n",
        "            logger.info(f\"Found website URL for {company}: {website_url}\")\n",
        "\n",
        "            # Find terms and conditions\n",
        "            terms_url = find_terms_link(website_url)\n",
        "            if not terms_url:\n",
        "                continue\n",
        "\n",
        "            logger.info(f\"Found terms and conditions URL for {company}: {terms_url}\")\n",
        "\n",
        "            # Download terms\n",
        "            download_terms(terms_url, company)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error processing {company}: {e}\")\n",
        "            continue\n",
        "\n",
        "        finally:\n",
        "            # Always respect rate limits\n",
        "            time.sleep(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "c2RKeCMX6TjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the 'terms' folder into a file called 'terms.zip'\n",
        "shutil.make_archive('terms', 'zip', 'terms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tDeece8e60al",
        "outputId": "4fbb89ad-6359-4f10-b369-9b5954e552b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/terms.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import json\n",
        "from typing import Optional, Dict\n",
        "import time\n",
        "from datetime import datetime\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('scraper.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CredentialManager:\n",
        "    \"\"\"Manages API credentials from Google Colab userdata\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_credentials() -> Dict[str, str]:\n",
        "        \"\"\"Retrieve and validate API credentials\"\"\"\n",
        "        try:\n",
        "            credentials = {\n",
        "                'google_api_key': userdata.get('GOOGLE_API_KEY'),\n",
        "                'search_engine_id': userdata.get('SEARCH_ENGINE_ID')\n",
        "            }\n",
        "\n",
        "            # Validate credentials\n",
        "            if not all(credentials.values()):\n",
        "                missing = [k for k, v in credentials.items() if not v]\n",
        "                raise ValueError(f\"Missing required credentials: {', '.join(missing)}\")\n",
        "\n",
        "            return credentials\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error retrieving credentials: {e}\")\n",
        "            raise\n",
        "\n",
        "class ScrapingProgress:\n",
        "    def __init__(self, progress_file: str = \"scraping_progress.json\"):\n",
        "        self.progress_file = Path(progress_file)\n",
        "        self.progress_data = self._load_progress()\n",
        "\n",
        "    def _load_progress(self) -> Dict:\n",
        "        \"\"\"Load progress data from file or create new if doesn't exist\"\"\"\n",
        "        if self.progress_file.exists():\n",
        "            try:\n",
        "                with open(self.progress_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                logger.error(\"Corrupted progress file, creating new one\")\n",
        "                return self._create_new_progress()\n",
        "        return self._create_new_progress()\n",
        "\n",
        "    def _create_new_progress(self) -> Dict:\n",
        "        \"\"\"Create new progress tracking structure\"\"\"\n",
        "        return {\n",
        "            'last_index': None,\n",
        "            'last_company': None,\n",
        "            'timestamp': None,\n",
        "            'completed_companies': [],\n",
        "            'failed_companies': []\n",
        "        }\n",
        "\n",
        "    def save_progress(self, index: int, company: str, success: bool):\n",
        "        \"\"\"Save current progress to file\"\"\"\n",
        "        self.progress_data['last_index'] = index\n",
        "        self.progress_data['last_company'] = company\n",
        "        self.progress_data['timestamp'] = datetime.now().isoformat()\n",
        "\n",
        "        if success:\n",
        "            if company not in self.progress_data['completed_companies']:\n",
        "                self.progress_data['completed_companies'].append(company)\n",
        "        else:\n",
        "            if company not in self.progress_data['failed_companies']:\n",
        "                self.progress_data['failed_companies'].append(company)\n",
        "\n",
        "        with open(self.progress_file, 'w') as f:\n",
        "            json.dump(self.progress_data, f, indent=2)\n",
        "\n",
        "    def get_last_index(self) -> Optional[int]:\n",
        "        \"\"\"Get the last processed index\"\"\"\n",
        "        return self.progress_data['last_index']\n",
        "\n",
        "def search_company_website(company_name: str, credentials: Dict[str, str]) -> Optional[str]:\n",
        "    \"\"\"Search for company website using Google Custom Search API\"\"\"\n",
        "    try:\n",
        "        service = build(\"customsearch\", \"v1\", developerKey=credentials['google_api_key'])\n",
        "        res = service.cse().list(\n",
        "            q=f\"{company_name} official website\",\n",
        "            cx=credentials['search_engine_id'],\n",
        "            num=1\n",
        "        ).execute()\n",
        "\n",
        "        if 'items' in res:\n",
        "            url = res['items'][0]['link']\n",
        "            parsed_url = urlparse(url)\n",
        "            return f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "        logger.warning(f\"No results found for {company_name}\")\n",
        "        return None\n",
        "    except HttpError as e:\n",
        "        logger.error(f\"Google API error for {company_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "def main(start_company: Optional[str] = None, resume: bool = True):\n",
        "    \"\"\"\n",
        "    Main execution function with resume capability\n",
        "\n",
        "    Args:\n",
        "        start_company: Specific company to start from (optional)\n",
        "        resume: Whether to resume from last known position\n",
        "    \"\"\"\n",
        "    # Initialize progress tracking and get credentials\n",
        "    progress = ScrapingProgress()\n",
        "\n",
        "    try:\n",
        "        # Get and validate credentials\n",
        "        credentials = CredentialManager.get_credentials()\n",
        "\n",
        "        # Get S&P 500 companies\n",
        "        sp_500_companies = get_sp500_companies()\n",
        "        if not sp_500_companies:\n",
        "            logger.error(\"Failed to fetch S&P 500 companies list\")\n",
        "            return\n",
        "\n",
        "        # Determine starting point\n",
        "        start_index = 0\n",
        "        if start_company:\n",
        "            try:\n",
        "                start_index = sp_500_companies.index(start_company)\n",
        "                logger.info(f\"Starting from specified company: {start_company} (index: {start_index})\")\n",
        "                sp_500_companies = sp_500_companies[start_index:]\n",
        "            except ValueError:\n",
        "                logger.error(f\"{start_company} not found in the list of S&P 500 companies.\")\n",
        "                return\n",
        "        elif resume:\n",
        "            last_index = progress.get_last_index()\n",
        "            if last_index is not None:\n",
        "                start_index = last_index + 1\n",
        "                logger.info(f\"Resuming from index: {start_index}\")\n",
        "                sp_500_companies = sp_500_companies[start_index:]\n",
        "\n",
        "        # Create output directory if it doesn't exist\n",
        "        Path('terms').mkdir(exist_ok=True)\n",
        "\n",
        "        # Process companies\n",
        "        for index, company in enumerate(sp_500_companies, start=start_index):\n",
        "            try:\n",
        "                logger.info(f\"Processing {company} (Index: {index})...\")\n",
        "\n",
        "                # Check if company was already processed\n",
        "                if company in progress.progress_data['completed_companies']:\n",
        "                    logger.info(f\"Skipping {company} - already processed\")\n",
        "                    continue\n",
        "\n",
        "                # Search for company website\n",
        "                website_url = search_company_website(company, credentials)\n",
        "                if not website_url:\n",
        "                    logger.warning(f\"No website found for {company}\")\n",
        "                    progress.save_progress(index, company, False)\n",
        "                    continue\n",
        "\n",
        "                logger.info(f\"Found website URL for {company}: {website_url}\")\n",
        "\n",
        "                # Find terms and conditions\n",
        "                terms_url = find_terms_link(website_url)\n",
        "                if not terms_url:\n",
        "                    logger.warning(f\"No terms and conditions link found for {company}\")\n",
        "                    progress.save_progress(index, company, False)\n",
        "                    continue\n",
        "\n",
        "                logger.info(f\"Found terms and conditions URL for {company}: {terms_url}\")\n",
        "\n",
        "                # Download and sanitize terms\n",
        "                download_terms(terms_url, company)\n",
        "                progress.save_progress(index, company, True)\n",
        "                logger.info(f\"Successfully processed {company} (Index: {index})\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing {company}: {e}\", exc_info=True)\n",
        "                progress.save_progress(index, company, False)\n",
        "                continue\n",
        "\n",
        "            finally:\n",
        "                # Always respect rate limits\n",
        "                time.sleep(1)\n",
        "\n",
        "        # Log final statistics\n",
        "        logger.info(\"Scraping completed!\")\n",
        "        logger.info(f\"Successfully processed: {len(progress.progress_data['completed_companies'])} companies\")\n",
        "        logger.info(f\"Failed to process: {len(progress.progress_data['failed_companies'])} companies\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Critical error in main execution: {e}\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    main(start_company=\"eBay\", resume=True)"
      ],
      "metadata": {
        "id": "TfplEeNA7I45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the 'terms' folder into a file called 'terms.zip'\n",
        "shutil.make_archive('terms', 'zip', 'terms')"
      ],
      "metadata": {
        "id": "ASnHWjvO7LZO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}